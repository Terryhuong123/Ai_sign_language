import cv2
import mediapipe as mp
import numpy as np
import keras
from collections import deque
import time

# ==================== è¨­å®šåƒæ•¸ ====================
MODEL_PATH = 'processed_data/model/sign_language_lstm.keras'
LABEL_ENCODER_PATH = 'processed_data/model/label_encoder.npy'
MAX_FRAMES = 30  # ç´¯ç©å¹€æ•¸
CONFIDENCE_THRESHOLD = 0.3  # ä¿¡å¿ƒåº¦é–€æª»

# ==================== è¼‰å…¥æ¨¡å‹ ====================
print("è¼‰å…¥æ¨¡å‹...")
model = keras.models.load_model(MODEL_PATH)
label_classes = np.load(LABEL_ENCODER_PATH, allow_pickle=True)
print(f"âœ“ æ¨¡å‹å·²è¼‰å…¥ï¼Œå…± {len(label_classes)} å€‹é¡åˆ¥")

# ==================== åˆå§‹åŒ– MediaPipe ====================
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

# ==================== è¼”åŠ©å‡½æ•¸ ====================
def extract_keypoints_from_frame(results):
    """å¾å–®å¹€ä¸­æå–é—œéµé»"""
    left_hand = [0.0] * 63
    right_hand = [0.0] * 63
    
    if results.multi_hand_landmarks and results.multi_handedness:
        for idx, hand_landmarks in enumerate(results.multi_hand_landmarks):
            handedness = results.multi_handedness[idx].classification[0].label
            
            keypoints = []
            for landmark in hand_landmarks.landmark:
                keypoints.extend([landmark.x, landmark.y, landmark.z])
            
            if handedness == "Left":
                left_hand = keypoints
            else:
                right_hand = keypoints
    
    combined = left_hand + right_hand
    return combined


def draw_ui(frame, prediction_text, confidence, top_predictions, frame_count, is_recording):
    """ç¹ªè£½ä½¿ç”¨è€…ä»‹é¢"""
    h, w = frame.shape[:2]
    
    # åŠé€æ˜èƒŒæ™¯
    overlay = frame.copy()
    cv2.rectangle(overlay, (10, 10), (w-10, 180), (0, 0, 0), -1)
    cv2.addWeighted(overlay, 0.6, frame, 0.4, 0, frame)
    
    # æ¨™é¡Œ
    cv2.putText(frame, "Sign Language Recognition System", (20, 40),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)
    
    # éŒ„è£½ç‹€æ…‹
    if is_recording:
        status_text = f"Recording: {frame_count}/{MAX_FRAMES}"
        color = (0, 255, 0)  # ç¶ è‰²
    else:
        status_text = f"Waiting... ({frame_count}/{MAX_FRAMES})"
        color = (100, 100, 100)  # ç°è‰²
    
    cv2.putText(frame, status_text, (20, 70),
                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)
    
    # é æ¸¬çµæœ
    if prediction_text and confidence >= CONFIDENCE_THRESHOLD:
        # ä¸»è¦é æ¸¬
        cv2.putText(frame, f"Prediction: {prediction_text}", (20, 110),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        cv2.putText(frame, f"Confidence: {confidence:.1%}", (20, 140),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)
        
        # Top-3 é æ¸¬
        y_offset = 200
        cv2.putText(frame, "Top 3 Predictions:", (20, y_offset),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 1)
        
        for i, (label, conf) in enumerate(top_predictions[:3], 1):
            y_offset += 30
            text = f"{i}. {label} ({conf:.1%})"
            cv2.putText(frame, text, (30, y_offset),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    elif prediction_text:
        cv2.putText(frame, "Low Confidence - Keep Signing", (20, 110),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)
    
    # æ“ä½œèªªæ˜
    instructions = [
        "Controls:",
        "SPACE - Start/Stop Recording",
        "R - Reset",
        "Q - Quit"
    ]
    
    y_start = h - 120
    for i, text in enumerate(instructions):
        cv2.putText(frame, text, (20, y_start + i*25),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
    
    return frame


def predict_sign(keypoints_sequence):
    """é æ¸¬æ‰‹èª"""
    # å¡«å……æˆ–æˆªæ–·åˆ° MAX_FRAMES
    keypoints_array = np.array(keypoints_sequence)
    
    if len(keypoints_array) < MAX_FRAMES:
        padding = np.zeros((MAX_FRAMES - len(keypoints_array), 126))
        keypoints_array = np.vstack([keypoints_array, padding])
    else:
        keypoints_array = keypoints_array[:MAX_FRAMES]
    
    # é æ¸¬
    keypoints_array = keypoints_array.reshape(1, MAX_FRAMES, 126)
    predictions = model.predict(keypoints_array, verbose=0)[0]
    
    # å–å¾— Top-3 é æ¸¬
    top_indices = np.argsort(predictions)[-3:][::-1]
    top_predictions = [(label_classes[i], predictions[i]) for i in top_indices]
    
    # æœ€ä½³é æ¸¬
    predicted_class = np.argmax(predictions)
    confidence = predictions[predicted_class]
    predicted_label = label_classes[predicted_class]
    
    return predicted_label, confidence, top_predictions


# ==================== ä¸»ç¨‹å¼ ====================
def main():
    cap = cv2.VideoCapture(0)
    
    if not cap.isOpened():
        print("âŒ ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ")
        return
    
    # è¨­å®šæ”å½±æ©Ÿè§£æåº¦
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    
    print("\n" + "="*60)
    print("ğŸ¥ æ‰‹èªè¾¨è­˜ç³»çµ±å·²å•Ÿå‹•")
    print("="*60)
    print("æ“ä½œèªªæ˜ï¼š")
    print("  [ç©ºç™½éµ] - é–‹å§‹/åœæ­¢éŒ„è£½")
    print("  [R] - é‡ç½®")
    print("  [Q] - é€€å‡º")
    print("="*60 + "\n")
    
    keypoints_buffer = deque(maxlen=MAX_FRAMES)
    is_recording = False
    prediction_text = ""
    confidence = 0.0
    top_predictions = []
    frame_count = 0
    last_prediction_time = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            print("âŒ ç„¡æ³•è®€å–å½±åƒ")
            break
        
        # æ°´å¹³ç¿»è½‰ï¼ˆé¡åƒï¼‰
        frame = cv2.flip(frame, 1)
        
        # è½‰æ›ç‚º RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        
        # åµæ¸¬æ‰‹éƒ¨
        results = hands.process(frame_rgb)
        
        # ç¹ªè£½æ‰‹éƒ¨é—œéµé»
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                mp_drawing.draw_landmarks(
                    frame,
                    hand_landmarks,
                    mp_hands.HAND_CONNECTIONS,
                    mp_drawing_styles.get_default_hand_landmarks_style(),
                    mp_drawing_styles.get_default_hand_connections_style()
                )
            
            # éŒ„è£½æ¨¡å¼
            if is_recording:
                keypoints = extract_keypoints_from_frame(results)
                keypoints_buffer.append(keypoints)
                frame_count = len(keypoints_buffer)
                
                # é”åˆ° MAX_FRAMES æ™‚è‡ªå‹•é æ¸¬
                if frame_count >= MAX_FRAMES:
                    current_time = time.time()
                    
                    # é¿å…éæ–¼é »ç¹é æ¸¬ï¼ˆæ¯ 1 ç§’æœ€å¤šé æ¸¬ä¸€æ¬¡ï¼‰
                    if current_time - last_prediction_time > 1.0:
                        prediction_text, confidence, top_predictions = predict_sign(list(keypoints_buffer))
                        last_prediction_time = current_time
                        
                        print(f"\né æ¸¬çµæœ: {prediction_text} ({confidence:.1%})")
                        for i, (label, conf) in enumerate(top_predictions, 1):
                            print(f"  {i}. {label}: {conf:.1%}")
                    
                    # æ¸…ç©º buffer æº–å‚™ä¸‹æ¬¡éŒ„è£½
                    keypoints_buffer.clear()
                    frame_count = 0
                    is_recording = False
        
        # ç¹ªè£½ UI
        frame = draw_ui(frame, prediction_text, confidence, top_predictions, 
                       frame_count, is_recording)
        
        # é¡¯ç¤ºç•«é¢
        cv2.imshow('Sign Language Recognition', frame)
        
        # éµç›¤æ§åˆ¶
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q') or key == 27:  # Q æˆ– ESC
            print("\né€€å‡ºç³»çµ±...")
            break
        
        elif key == ord(' '):  # ç©ºç™½éµ
            if not is_recording:
                print("\né–‹å§‹éŒ„è£½æ‰‹èªå‹•ä½œ...")
                keypoints_buffer.clear()
                frame_count = 0
                is_recording = True
                prediction_text = ""
            else:
                print("åœæ­¢éŒ„è£½")
                is_recording = False
        
        elif key == ord('r') or key == ord('R'):  # R
            print("\né‡ç½®")
            keypoints_buffer.clear()
            frame_count = 0
            is_recording = False
            prediction_text = ""
            confidence = 0.0
            top_predictions = []
    
    # æ¸…ç†
    cap.release()
    cv2.destroyAllWindows()
    hands.close()
    print("\nâœ“ ç³»çµ±å·²é—œé–‰")


if __name__ == "__main__":
    main()