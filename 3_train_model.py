import numpy as np
import keras
from keras import layers
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import os
from collections import Counter

# è¼‰å…¥è³‡æ–™
print("è¼‰å…¥è³‡æ–™...")
keypoints = np.load('processed_data/keypoints.npy')
labels = np.load('processed_data/labels.npy')

print(f"åŸå§‹é—œéµé»å½¢ç‹€: {keypoints.shape}")
print(f"åŸå§‹æ¨™ç±¤æ•¸é‡: {len(labels)}")

# çµ±è¨ˆæ¯å€‹æ¨™ç±¤å‡ºç¾çš„æ¬¡æ•¸
label_counts = Counter(labels)
print(f"\næ¨™ç±¤çµ±è¨ˆ:")
print(f"  ç¸½å…± {len(label_counts)} å€‹ä¸åŒçš„å¥å­")
print(f"  åªå‡ºç¾ 1 æ¬¡çš„å¥å­: {sum(1 for c in label_counts.values() if c == 1)} å€‹")
print(f"  å‡ºç¾ 2 æ¬¡çš„å¥å­: {sum(1 for c in label_counts.values() if c == 2)} å€‹")
print(f"  å‡ºç¾ â‰¥ 3 æ¬¡çš„å¥å­: {sum(1 for c in label_counts.values() if c >= 3)} å€‹")
print(f"  å‡ºç¾ â‰¥ 4 æ¬¡çš„å¥å­: {sum(1 for c in label_counts.values() if c >= 4)} å€‹")

# âœ… ä¿®æ­£ï¼šåªä¿ç•™å‡ºç¾æ¬¡æ•¸ >= 3 çš„é¡åˆ¥
min_samples = 3
valid_labels = [label for label, count in label_counts.items() if count >= min_samples]
valid_indices = [i for i, label in enumerate(labels) if label in valid_labels]

keypoints_filtered = keypoints[valid_indices]
labels_filtered = labels[valid_indices]

print(f"\néæ¿¾å¾Œ (min_samples={min_samples}):")
print(f"  ä¿ç•™æ¨£æœ¬æ•¸: {len(keypoints_filtered)}")
print(f"  ä¿ç•™é¡åˆ¥æ•¸: {len(set(labels_filtered))}")

# æª¢æŸ¥æ˜¯å¦æœ‰è¶³å¤ çš„è³‡æ–™
num_unique_classes = len(set(labels_filtered))
num_samples = len(keypoints_filtered)
test_size_ratio = 0.2
estimated_test_size = int(num_samples * test_size_ratio)

print(f"\nå¯è¡Œæ€§æª¢æŸ¥:")
print(f"  é è¨ˆæ¸¬è©¦é›†å¤§å°: {estimated_test_size}")
print(f"  é¡åˆ¥æ•¸: {num_unique_classes}")

if estimated_test_size < num_unique_classes:
    print(f"  âš ï¸ æ¸¬è©¦é›†å¤ªå°ï¼èª¿æ•´ test_size...")
    # è¨ˆç®—æœ€å°çš„ test_size
    min_test_size = num_unique_classes / num_samples
    test_size_ratio = max(min_test_size * 1.2, 0.25)  # è‡³å°‘ 25%
    print(f"  æ–°çš„ test_size: {test_size_ratio:.2f}")

# ç·¨ç¢¼æ¨™ç±¤
label_encoder = LabelEncoder()
labels_encoded = label_encoder.fit_transform(labels_filtered)
num_classes = len(label_encoder.classes_)

print(f"\næœ€çµ‚è©å½™é¡åˆ¥æ•¸: {num_classes}")
print(f"è©å½™åˆ—è¡¨ (å‰ 20 å€‹): {label_encoder.classes_[:20]}")

# é¡¯ç¤ºæ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸çµ±è¨ˆ
class_counts = Counter(labels_filtered)
counts_distribution = Counter(class_counts.values())
print(f"\næ¯å€‹é¡åˆ¥çš„æ¨£æœ¬æ•¸åˆ†å¸ƒ:")
for count, num_classes_with_count in sorted(counts_distribution.items()):
    print(f"  {count} å€‹æ¨£æœ¬: {num_classes_with_count} å€‹é¡åˆ¥")

# åˆ†å‰²è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†
X_train, X_test, y_train, y_test = train_test_split(
    keypoints_filtered, 
    labels_encoded, 
    test_size=test_size_ratio,
    random_state=42,
    stratify=labels_encoded
)

print(f"\nè¨“ç·´é›†å¤§å°: {X_train.shape}")
print(f"æ¸¬è©¦é›†å¤§å°: {X_test.shape}")

# å»ºç«‹ LSTM æ¨¡å‹
def create_lstm_model(input_shape, num_classes):
    model = keras.Sequential([
        layers.Input(shape=input_shape),
        
        # LSTM å±¤
        layers.LSTM(128, return_sequences=True),
        layers.Dropout(0.4),
        
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(0.4),
        
        layers.LSTM(32),
        layers.Dropout(0.4),
        
        # å…¨é€£æ¥å±¤
        layers.Dense(64, activation='relu'),
        layers.Dropout(0.4),
        
        # è¼¸å‡ºå±¤
        layers.Dense(num_classes, activation='softmax')
    ])
    
    return model

# å»ºç«‹æ¨¡å‹
input_shape = (keypoints_filtered.shape[1], keypoints_filtered.shape[2])
model = create_lstm_model(input_shape, num_classes)

# ç·¨è­¯æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

# è¨“ç·´æ¨¡å‹
print("\né–‹å§‹è¨“ç·´...")
print("=" * 60)

history = model.fit(
    X_train, y_train,
    validation_data=(X_test, y_test),
    epochs=100,
    batch_size=16,
    callbacks=[
        keras.callbacks.EarlyStopping(
            monitor='val_loss',
            patience=20,
            restore_best_weights=True,
            verbose=1
        ),
        keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            patience=8,
            factor=0.5,
            verbose=1
        )
    ],
    verbose=1
)

# è©•ä¼°æ¨¡å‹
print("\nè©•ä¼°æ¨¡å‹...")
test_loss, test_acc = model.evaluate(X_test, y_test)

print(f"\n{'='*60}")
print(f"ğŸ¯ æœ€çµ‚çµæœ")
print(f"{'='*60}")
print(f"æ¸¬è©¦æº–ç¢ºç‡: {test_acc:.2%}")
print(f"æ¸¬è©¦æå¤±: {test_loss:.4f}")
print(f"è¨“ç·´ Epochs: {len(history.history['loss'])}")
print(f"æœ€ä½³é©—è­‰æº–ç¢ºç‡: {max(history.history['val_accuracy']):.2%}")
print(f"{'='*60}")

# å„²å­˜æ¨¡å‹
os.makedirs('processed_data/model', exist_ok=True)
model.save('processed_data/model/sign_language_lstm.keras')
np.save('processed_data/model/label_encoder.npy', label_encoder.classes_)

print("\nâœ“ æ¨¡å‹å·²å„²å­˜è‡³ processed_data/model/")

# ç¹ªè£½è¨“ç·´æ›²ç·š
plt.rcParams['font.sans-serif'] = ['Microsoft JhengHei', 'SimHei', 'Arial Unicode MS']
plt.rcParams['axes.unicode_minus'] = False

plt.figure(figsize=(14, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Accuracy', fontsize=12)
plt.legend(fontsize=10)
plt.title('Model Accuracy', fontsize=14)
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', linewidth=2)
plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
plt.xlabel('Epoch', fontsize=12)
plt.ylabel('Loss', fontsize=12)
plt.legend(fontsize=10)
plt.title('Model Loss', fontsize=14)
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('processed_data/model/training_history.png', dpi=150, bbox_inches='tight')
print("âœ“ è¨“ç·´æ›²ç·šå·²å„²å­˜")

# Top-5 æº–ç¢ºç‡ï¼ˆæ›´å¯¬é¬†çš„æŒ‡æ¨™ï¼‰
print("\nè¨ˆç®— Top-5 æº–ç¢ºç‡...")
y_pred_proba = model.predict(X_test, verbose=0)
top5_predictions = np.argsort(y_pred_proba, axis=1)[:, -5:]  # å–å‰ 5 å€‹é æ¸¬
top5_accuracy = np.mean([y_test[i] in top5_predictions[i] for i in range(len(y_test))])
print(f"Top-5 æº–ç¢ºç‡: {top5_accuracy:.2%}")